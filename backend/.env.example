# =============================================================================
# LLM API Configuration (OpenAI-compatible)
# This app works with ANY OpenAI API-compatible provider!
# =============================================================================

# Primary LLM Provider
# ---------------------
# Ollama (default, runs locally)
LLM_BASE_URL=http://localhost:11434/v1
LLM_API_KEY=ollama
LLM_MODEL=llama2

# Alternative: OpenAI
# LLM_BASE_URL=https://api.openai.com/v1
# LLM_API_KEY=sk-your-openai-api-key
# LLM_MODEL=gpt-3.5-turbo

# Alternative: Anthropic (via proxy)
# LLM_BASE_URL=https://api.anthropic.com/v1
# LLM_API_KEY=sk-ant-your-key
# LLM_MODEL=claude-3-haiku-20240307

# Alternative: LM Studio
# LLM_BASE_URL=http://localhost:1234/v1
# LLM_API_KEY=lm-studio
# LLM_MODEL=your-loaded-model

# Alternative: Groq
# LLM_BASE_URL=https://api.groq.com/openai/v1
# LLM_API_KEY=gsk_your-groq-key
# LLM_MODEL=llama3-8b-8192

# Fallback LLM Provider (optional)
# --------------------------------
# If the primary provider fails, the app will try this one
# LLM_FALLBACK_BASE_URL=https://api.openai.com/v1
# LLM_FALLBACK_API_KEY=sk-your-openai-key
# LLM_FALLBACK_MODEL=gpt-3.5-turbo

# =============================================================================
# Whisper Configuration (local speech-to-text)
# =============================================================================
# Available models: tiny, tiny.en, base, base.en, small, small.en, medium, medium.en, large-v3
# Smaller models are faster but less accurate
WHISPER_MODEL=base.en

# =============================================================================
# Embedding Configuration (for RAG)
# =============================================================================
# Ollama embedding endpoint (note: no /v1 suffix for embeddings API)
# If not set, defaults to LLM_BASE_URL with /v1 stripped
EMBEDDING_BASE_URL=http://localhost:11434
EMBEDDING_MODEL=nomic-embed-text

# To use RAG, pull the embedding model first:
#   ollama pull nomic-embed-text

# =============================================================================
# Server Configuration
# =============================================================================
# LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR

# =============================================================================
# In devcontainer, Ollama runs as a separate Docker service
# The base URL should be: http://ollama:11434/v1
# =============================================================================
