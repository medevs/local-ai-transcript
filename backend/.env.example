# =============================================================================
# LLM API Configuration (OpenAI-compatible)
# This app works with ANY OpenAI API-compatible provider!
# =============================================================================

# Primary LLM Provider
# ---------------------
# Ollama (default, runs locally)
LLM_BASE_URL=http://localhost:11434/v1
LLM_API_KEY=ollama
LLM_MODEL=llama2

# Alternative: OpenAI
# LLM_BASE_URL=https://api.openai.com/v1
# LLM_API_KEY=sk-your-openai-api-key
# LLM_MODEL=gpt-3.5-turbo

# Alternative: Anthropic (via proxy)
# LLM_BASE_URL=https://api.anthropic.com/v1
# LLM_API_KEY=sk-ant-your-key
# LLM_MODEL=claude-3-haiku-20240307

# Alternative: LM Studio
# LLM_BASE_URL=http://localhost:1234/v1
# LLM_API_KEY=lm-studio
# LLM_MODEL=your-loaded-model

# Alternative: Groq
# LLM_BASE_URL=https://api.groq.com/openai/v1
# LLM_API_KEY=gsk_your-groq-key
# LLM_MODEL=llama3-8b-8192

# Fallback LLM Provider (optional)
# --------------------------------
# If the primary provider fails, the app will try this one
# LLM_FALLBACK_BASE_URL=https://api.openai.com/v1
# LLM_FALLBACK_API_KEY=sk-your-openai-key
# LLM_FALLBACK_MODEL=gpt-3.5-turbo

# =============================================================================
# Whisper Configuration (local speech-to-text)
# =============================================================================
# Available models: tiny, tiny.en, base, base.en, small, small.en, medium, medium.en, large-v3
# Smaller models are faster but less accurate
WHISPER_MODEL=base.en

# =============================================================================
# Embedding Configuration (for RAG)
# =============================================================================
# Ollama embedding endpoint (note: no /v1 suffix for embeddings API)
# If not set, defaults to LLM_BASE_URL with /v1 stripped
EMBEDDING_BASE_URL=http://localhost:11434
EMBEDDING_MODEL=nomic-embed-text

# RAG chunking settings
# CHUNK_SIZE=500           # Characters per chunk
# CHUNK_OVERLAP=100        # Overlap between chunks
# EMBEDDING_DIM=768        # Embedding dimension (nomic-embed-text = 768)
# TOP_K_CHUNKS=5           # Number of chunks to retrieve for context

# To use RAG, pull the embedding model first:
#   ollama pull nomic-embed-text

# =============================================================================
# Server Configuration
# =============================================================================
# LOG_LEVEL=INFO           # DEBUG, INFO, WARNING, ERROR

# CORS origins (comma-separated)
# CORS_ORIGINS=http://localhost:3000,http://localhost:5173

# =============================================================================
# File Upload Configuration
# =============================================================================
# MAX_UPLOAD_SIZE_MB=100   # Maximum upload size in MB

# Allowed audio MIME types (comma-separated, defaults to common audio types)
# ALLOWED_AUDIO_TYPES=audio/webm,audio/wav,audio/mp3,audio/mpeg,audio/ogg,audio/flac

# =============================================================================
# Database Configuration
# =============================================================================
# DATABASE_PATH=data/transcripts.db

# =============================================================================
# Rate Limiting
# =============================================================================
# RATE_LIMIT_TRANSCRIBE=5/minute
# RATE_LIMIT_CLEAN=20/minute
# RATE_LIMIT_CHAT=30/minute

# =============================================================================
# Chat Configuration
# =============================================================================
# MAX_CHAT_HISTORY=10      # Maximum chat messages to include in context

# =============================================================================
# In devcontainer, Ollama runs as a separate Docker service
# The base URL should be: http://ollama:11434/v1
# =============================================================================
